{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pomegranate as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sequentia.classifiers import HMM, HMMClassifier, DTWKNN\n",
    "from sequentia.preprocessing import Preprocess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pylab import rcParams\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "rand = np.random.RandomState(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gestures = ['nd', 'mnd', 'fu', 'fd', 'sh', 't', 'ti']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recording:\n",
    "    def __init__(self, csv):\n",
    "        self._name = Path(csv).stem.split('.')[0]\n",
    "        self._speaker = self._name.split('_')[0]\n",
    "        self._personality = self._name.split('_')[-1]\n",
    "        self._path = csv\n",
    "        \n",
    "    @property\n",
    "    def id(self) -> str:\n",
    "        \"\"\"The recording identifier.\"\"\"\n",
    "        return self._name\n",
    "    \n",
    "    @property\n",
    "    def speaker(self) -> str:\n",
    "        \"\"\"The speaker of the recording.\"\"\"\n",
    "        return self._speaker\n",
    "    \n",
    "    @property\n",
    "    def path(self) -> str:\n",
    "        \"\"\"The path to the rov.csv parameters file.\"\"\"\n",
    "        return self._path\n",
    "\n",
    "class Parameters(Recording):\n",
    "    def __init__(self, csv):\n",
    "        super().__init__(csv)\n",
    "    \n",
    "    def as_df(self) -> pd.DataFrame:\n",
    "        \"\"\"Creates a dataframe from the rov.csv parameters file.\"\"\"\n",
    "        \n",
    "        # Load the annotations\n",
    "        df = pd.read_csv(self._path).astype(np.float64) \n",
    "        \n",
    "        # Select only the rotation vectors\n",
    "        df = df[['Rx', 'Ry', 'Rz']]\n",
    "        \n",
    "        # Add recording identifier column\n",
    "        df['recording'] = self._name\n",
    "        \n",
    "        return df\n",
    "\n",
    "class Annotations(Recording):\n",
    "    def __init__(self, csv):\n",
    "        self._params = Parameters(\n",
    "            re.sub('/rov.csv', '/Normalized/rov.csv', \n",
    "                   re.sub('eaf.csv', 'rov.csv', \n",
    "                          re.sub('annotations', 'params', csv)))\n",
    "        )\n",
    "        super().__init__(csv)\n",
    "    \n",
    "    @property\n",
    "    def params(self) -> Parameters:\n",
    "        \"\"\"Returns the corresponding {Parameters} object for the annotations.\"\"\"\n",
    "        return self._params        \n",
    "    \n",
    "    def as_df(self) -> pd.DataFrame:\n",
    "        \"\"\"Creates a dataframe from the eaf.csv annotations file.\"\"\"\n",
    "        \n",
    "        # Load the annotations\n",
    "        df = pd.read_csv(self._path)   \n",
    "        \n",
    "        # Select the gesture type, start-time, end-time and duration columns\n",
    "        df = df[['type', 'start_time', 'end_time', 'during_time']]\n",
    "        \n",
    "        # Rename 'during_time' column to 'duration'\n",
    "        df = df.rename(columns={'type': 'gesture', 'during_time': 'duration'})\n",
    "        \n",
    "        # Convert the 'duration' column to an integer\n",
    "        df = df.astype({'start_time': 'int32', 'end_time': 'int32', 'duration': 'int32'})\n",
    "\n",
    "        # Convert the units from milliseconds to frames (/1000 and *100)\n",
    "        df.loc[:, ['start_time', 'end_time', 'duration']] //= 10\n",
    "        \n",
    "        # Add recording identifier column\n",
    "        df['recording'] = self._name\n",
    "        \n",
    "        # Reorder columns such that 'gesture' is the last column\n",
    "        df = df[['start_time', 'end_time', 'duration', 'recording', 'gesture']]\n",
    "        \n",
    "        # Remove 'start' gestures\n",
    "        df = df[df['gesture'] != 'start']\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths to all annotation CSV files\n",
    "csvs = glob.glob('../annotations/*/eaf.csv/*.eaf.csv')\n",
    "\n",
    "# Initialize {Annotations} object for each eaf.csv annotations file\n",
    "anns = [Annotations(csv) for csv in csvs]\n",
    "\n",
    "# Combine the parameters dataframes for each {Annotations} object\n",
    "param_df = pd.concat(ann.params.as_df() for ann in anns)\n",
    "\n",
    "# Combine the dataframes for each {Annotations} object\n",
    "ann_df = pd.concat(ann.as_df() for ann in anns)\n",
    "\n",
    "# Shuffle the annotations dataframe\n",
    "ann_df = ann_df.reset_index(drop=True)\n",
    "\n",
    "# Remove blank gestures\n",
    "# NOTE: This may be caused by a larger problem, like recording misalignment - look into this!\n",
    "blank_idx = []\n",
    "for i, (_, gesture) in enumerate(ann_df.iterrows()):\n",
    "    start, end, _, recording, _ = gesture \n",
    "    gesture_df = param_df[param_df['recording'] == recording].iloc[start:end]\n",
    "    if gesture_df.empty:\n",
    "        blank_idx.append(i)  \n",
    "ann_df.drop(blank_idx, inplace=True)\n",
    "\n",
    "# Split the annotations dataframe into examples and labels\n",
    "ann_y = ann_df['gesture'] \n",
    "ann_X = ann_df.loc[:, ann_df.columns != 'gesture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe into lists of numpy arrays\n",
    "X = [param_df[param_df['recording'] == row['recording']].iloc[row['start_time']:row['end_time']][['Rx', 'Ry', 'Rz']].to_numpy() for _, row in ann_X.iterrows()]\n",
    "y = list(ann_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gesture(O, figsize=(12, 7), title=None):\n",
    "    title = 'Normalized head rotation vectors for a gesture' if title is None else title\n",
    "    labels = ['X (Roll)', 'Y (Yaw)', 'Z (Pitch)']\n",
    "    colors = ['blue', 'red', 'green']\n",
    "    fig, axs = plt.subplots(3, 1, sharex=True, figsize=figsize)\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        ax.plot(O[:, i], label=labels[i], color=colors[i])\n",
    "        ax.legend(loc='upper right')\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(hspace=0, wspace=0, top=0.92)\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    plt.show()\n",
    "    \n",
    "plot_gesture(X[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = Preprocess()\n",
    "pre.standardize()\n",
    "pre.filtrate(n=15, method='mean')\n",
    "pre.downsample(n=5)\n",
    "pre.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gesture(pre.transform(X[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample the dataset\n",
    "X = pre.transform(X)\n",
    "\n",
    "# Split the data into training, validation and test sets (65-20-15)\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.35, shuffle=True, random_state=rand)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, test_size=0.43, shuffle=True, random_state=rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display prior counts for each class\n",
    "classes, values = zip(*Counter(y_train).items())\n",
    "indexes = np.arange(len(classes))\n",
    "plt.bar(indexes, values, 0.75)\n",
    "plt.xticks(indexes, classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set size: {}'.format(len(y_train)))\n",
    "print('Validation set size: {}'.format(len(y_val)))\n",
    "print('Test set size: {}'.format(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for displaying results (accuracy and confusion matrix)\n",
    "def show_results(acc, cm, dataset, labels):\n",
    "    df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(df, annot=True)\n",
    "    plt.title('Confusion matrix for {} set predictions'.format(dataset), fontsize=14)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    # Fix for matplotlib bug that cuts off top/bottom of seaborn visualizations\n",
    "    b, t = plt.ylim()\n",
    "    plt.ylim(b + 0.5, t - 0.5)\n",
    "    plt.show()\n",
    "    print('Accuracy: {:.2f}%'.format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DTWKNN(k=1, radius=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, cm = clf.evaluate(X_val[:20], y_val[:20], labels=gestures, n_jobs=-1)\n",
    "show_results(acc, cm, dataset='validation', labels=gestures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesture model class\n",
    "\n",
    "A gesture model $\\lambda_i$ is represented by a HMM which is trained on the observations of a particular gesture.\n",
    "\n",
    "- **Emission distributions**: The emission distribution for each state can be modeled as a multivariate Gaussian over each of the rotation axes. `pg.MultivariateGaussianDistribution` can be used to specify a multivariate Gaussian distribution with some confusion matrix and mean vector. If we assume that each rotation axis is independent, instead of using a `pg.MultivariateGaussianDistribution` with a diagonal covariance matrix of rotation axis variances, we define the distribution as a `pg.IndependentComponentsDistribution` composed of one `pg.NormalDistribution` for each rotation axisâ€“this is because it appears that the parameters of a `pg.MultivariateGaussianDistribution` are learned during the Baum-Welch training algorithm, which may not be what we want. This is unclear because the only other alternative to setting the parameters would be to take the mean and covariances of the rotation axes over all observation examples in the training data, but this treats all of the training data as one long observation, which seems incorrect. Another potential problem with this model is that the same emission distribution is used for each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the gesture models\n",
    "\n",
    "One gesture model $\\lambda_i=(A_i,B_i,\\pi_i)$ is initialized and trained for each of the gestures: `nd`, `mnd`, `sh`, `fd`, `t`, `ti`, `fu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for gesture in gestures:\n",
    "    model = HMM(label=gesture, n_states=7, random_state=rand)\n",
    "    model.set_uniform_initial()\n",
    "    model.set_random_transitions()\n",
    "    model.fit([X_train[i] for i, y_i in enumerate(y_train) if y_i == gesture])\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = HMMClassifier()\n",
    "clf.fit(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, cm = clf.evaluate(X_val, y_val)\n",
    "show_results(acc, cm, dataset='validation', labels=gestures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(precision=3, suppress=True):\n",
    "    display(models[0].initial)\n",
    "    display(models[0].transitions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
